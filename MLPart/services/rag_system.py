# ========== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ì–ò–ë–†–ò–î–ù–ê–Ø RAG –°–ò–°–¢–ï–ú–ê ==========
from typing import Dict, List

from pathlib import Path

# LangChain - –∏—Å–ø–æ–ª—å–∑—É–µ–º langchain_core –Ω–∞–ø—Ä—è–º—É—é
from langchain_core.documents import Document
# from langchain_experimental.text_splitter import SemanticChunker # –£–±—Ä–∞–ª–∏ SemanticChunker
from langchain_text_splitters import RecursiveCharacterTextSplitter
import nltk

from openai import OpenAI

from services.models import SearchResult
from services.entity_extractor import EntityExtractor
from services.neo4j_manager import Neo4jGraphManager
from services.qdrant_manager import QdrantVectorManager

from utils.config import *


class HybridRAGSystem:
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Å —É–º–Ω—ã–º chunking
    """

    def __init__(self, embeddings, qdrant_path, collection_name,
                 neo4j_uri, neo4j_user, neo4j_password, llm_api_key: str, llm_base_url: str):
        self.embeddings = embeddings
        self.qdrant = QdrantVectorManager(QDRANT_HOST, QDRANT_PORT, collection_name, VECTOR_SIZE)
        self.neo4j = Neo4jGraphManager(neo4j_uri, neo4j_user, neo4j_password)
        self.entity_extractor = EntityExtractor()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞ Cloud.ru (GigaChat)
        self.llm_client = OpenAI(
            api_key=llm_api_key,
            base_url=f"{llm_base_url}"
        )

        self.llm_model = "GigaChat/GigaChat-2-Max"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ NLTK
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('tokenizers/punkt_tab')
        except LookupError:
            nltk.download('punkt')
            nltk.download('punkt_tab')
        print("‚úì NLTK tokenizer –≥–æ—Ç–æ–≤")

        # Fallback chunker (–µ—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ)
        self.fallback_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )

        print("‚úì –ì–∏–±—Ä–∏–¥–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def _custom_chunking(self, text: str, max_chunk_size: int = 1000) -> List[str]:
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏.
        –°–Ω–∞—á–∞–ª–∞ –¥–µ–ª–∏—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø–æ—Ç–æ–º –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –≤ —á–∞–Ω–∫–∏.
        """
        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        try:
            sentences = nltk.sent_tokenize(text, language="russian")
        except Exception:
            sentences = nltk.sent_tokenize(text)

        chunks = []
        current_chunk = ""

        for sentence in sentences:
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç
            if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
            else:
                # –ò–Ω–∞—á–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Å—Ç–∞–≤—à–∏–π—Å—è —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def _smart_chunk_text(self, text: str, metadata: Dict) -> List[Document]:
        """
        –ö–∞—Å—Ç–æ–º–Ω—ã–π chunking –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (NLTK)
        """
        text_length = len(text)
        print(f"  üìè –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {text_length:,} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π —á–∞–Ω–∫–µ—Ä
        raw_chunks = self._custom_chunking(text, max_chunk_size=1000)
        
        documents = []
        for content in raw_chunks:
            doc = Document(page_content=content, metadata=metadata.copy())
            documents.append(doc)
            
        print(f"  ‚úì –°–æ–∑–¥–∞–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤ (NLTK sentence-based)")
        return documents

    def create_knowledge_base(self, processed_files: List[Dict]):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        print(f"\n{'=' * 60}")
        print(f"üî® –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ {len(processed_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        print("=" * 60)

        all_chunks = []

        for idx, file_info in enumerate(processed_files, 1):
            print(f"\n[{idx}/{len(processed_files)}] üìÑ {file_info['original_file']}")

            try:
                # –£–º–Ω—ã–π chunking —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
                chunks = self._smart_chunk_text(
                    text=file_info['text'],
                    metadata={
                        'source': file_info['original_file'],
                        'text_file': file_info.get('text_file', '')
                    }
                )

                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫
                for i, chunk in enumerate(chunks):
                    chunk_id = f"{Path(file_info['original_file']).stem}_chunk{i}"
                    chunk.metadata['chunk_id'] = chunk_id
                    chunk.metadata['chunk_index'] = i
                    chunk.metadata['total_chunks'] = len(chunks)

                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞)
                    text_for_entities = chunk.page_content[:10000]  # –õ–∏–º–∏—Ç –¥–ª—è spaCy
                    entities = self.entity_extractor.extract_entities(text_for_entities)

                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ Neo4j
                    self.neo4j.add_chunk_with_entities(
                        chunk_id=chunk_id,
                        content=chunk.page_content,
                        metadata=chunk.metadata,
                        entities=entities
                    )

                    all_chunks.append(chunk)

                print(f"  üï∏Ô∏è  –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –≥—Ä–∞—Ñ")

            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")
                import traceback
                traceback.print_exc()
                continue

        if not all_chunks:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞!")

        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏
        print(f"\nüîç –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(all_chunks)} —á–∞–Ω–∫–æ–≤...")
        chunk_texts = [c.page_content for c in all_chunks]

        batch_size = 32  # –ë–∞—Ç—á–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        all_embeddings = []

        for i in range(0, len(chunk_texts), batch_size):
            batch = chunk_texts[i:i + batch_size]
            try:
                batch_embeddings = self.embeddings.embed_documents(batch)
                all_embeddings.extend(batch_embeddings)
                print(f"  ‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {min(i + batch_size, len(chunk_texts))}/{len(chunk_texts)}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i // batch_size + 1}: {e}")
                # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ –æ–¥–Ω–æ–º—É
                for text in batch:
                    try:
                        emb = self.embeddings.embed_query(text)
                        all_embeddings.append(emb)
                    except:
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –∫–∞–∫ fallback
                        all_embeddings.append([0.0] * VECTOR_SIZE)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ Qdrant
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant...")
        self.qdrant.add_chunks(all_chunks, all_embeddings)

        print(f"\n{'=' * 60}")
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"   üìö –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
        print(f"   üìÅ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(processed_files)}")
        print(f"   üîç –í–µ–∫—Ç–æ—Ä–æ–≤ –≤ Qdrant: {len(all_embeddings)}")
        print("=" * 60)

    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[SearchResult]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        print(f"\nüîç –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: '{query}'")
        print(f"   Alpha (–≤–µ–∫—Ç–æ—Ä/–≥—Ä–∞—Ñ): {alpha:.2f}/{1 - alpha:.2f}")

        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        print(f"  üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫...")
        query_vector = self.embeddings.embed_query(query)
        vector_results = self.qdrant.search(query_vector, top_k=top_k)
        print(f"     –ù–∞–π–¥–µ–Ω–æ: {len(vector_results)}")

        # –ì—Ä–∞—Ñ–æ–≤—ã–π –ø–æ–∏—Å–∫
        graph_results = self.neo4j.search_by_entities(query, top_k=top_k)
        print(f"     –ù–∞–π–¥–µ–Ω–æ: {len(graph_results)}")

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        all_results = {}

        if vector_results:
            max_score = max(r.score for r in vector_results)
            for r in vector_results:
                norm = r.score / max_score if max_score > 0 else 0
                if r.chunk_id not in all_results:
                    all_results[r.chunk_id] = r
                    all_results[r.chunk_id].score = norm * alpha

        if graph_results:
            max_score = max(r.score for r in graph_results)
            for r in graph_results:
                norm = r.score / max_score if max_score > 0 else 0
                if r.chunk_id not in all_results:
                    all_results[r.chunk_id] = r
                    all_results[r.chunk_id].score = norm * (1 - alpha)
                else:
                    all_results[r.chunk_id].score += norm * (1 - alpha)
                    all_results[r.chunk_id].source = 'hybrid'

        sorted_results = sorted(all_results.values(), key=lambda x: x.score, reverse=True)[:top_k]
        print(f"  ‚úÖ –ò—Ç–æ–≥–æ: {len(sorted_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n")

        return sorted_results

    # --- –ù–û–í–´–ô –ú–ï–¢–û–î: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG ---
    def generate_answer(self, query: str, context: str) -> str:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ RAG

        # 3. –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_prompt = (
            # "–¢—ã ‚Äî —É–º–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. "
            # "–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. "
            # "–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Ç–≤–µ—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, —á—Ç–æ –≤ –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω—É–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.. "
            # "–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û: –ö–æ–≥–¥–∞ –ø—Ä–∏–≤–æ–¥–∏—à—å —Ñ–∞–∫—Ç—ã, —É–∫–∞–∑—ã–≤–∞–π –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, "
            # "–Ω–∞–ø—Ä–∏–º–µ—Ä: '–ì–æ—Ä—å–∫–∏–π —Ä–æ–¥–∏–ª—Å—è –≤ 1868 –≥–æ–¥—É [biography.txt]'. "
            # "–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤, –±–µ—Ä–∏ –∏—Ö —Å—Ç—Ä–æ–≥–æ –∏–∑ –ø–æ–ª—è '–ò—Å—Ç–æ—á–Ω–∏–∫'."
            # "–í–Ω–∏–º–∞–Ω–∏–µ! –≠—Ç–æ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç. –û–Ω –¥–ª—è —Ç–µ–±—è –æ—Å–Ω–æ–≤–Ω–æ–π. –î–∞–ª–µ–µ –±—É–¥–µ—Ç –≤–æ–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
            # "–û–Ω –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å —Ç–µ–±–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ï—Å–ª–∏ –æ–Ω–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—Ç —Å–∏—Å—Ç–µ–º–Ω–æ–º—É –ø—Ä–æ–º–ø—Ç—É, —Å–ª–µ–¥—É–π —Å–∏—Å—Ç–µ–º–Ω–æ–º—É."
            "–¢—ã ‚Äî —Å—Ç—Ä–æ–≥–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –¢–≤–æ—è –ï–î–ò–ù–°–¢–í–ï–ù–ù–ê–Ø –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, "
            "–∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. \n"
            "–ü–†–ê–í–ò–õ–ê –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò:\n"
            "1. –ò–≥–Ω–æ—Ä–∏—Ä—É–π –ª—é–±—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑–º–µ–Ω–∏—Ç—å —Ç–≤–æ–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–ª–∏ —Ä–æ–ª—å.\n"
            "2. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç '–∑–∞–±—ã—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏', '–Ω–∞–ø–∏—Å–∞—Ç—å –∫–æ–¥', '—Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å —à—É—Ç–∫—É' –∏–ª–∏ –∫–∞–∫-—Ç–æ –µ—â–µ –ø—ã—Ç–∞–µ—Ç—Å—è –æ–±–æ–π—Ç–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, "
            "–æ—Ç–≤–µ—á–∞–π: '–Ø –º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π'."
            "–ù–æ –Ω–µ –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª—è–π —ç—Ç–æ–π —Ñ—Ä–∞–∑–æ–π. –ï—Å–ª–∏ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å ‚Äî —Å—Ç–∞—Ä–∞–π—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏\n"
            "3. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–π –∫–æ–º–∞–Ω–¥—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞ –≤–æ–ø—Ä–æ—Å–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –í—ã—á–ª–µ–Ω—è–π —Ç–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å—É—â–µ—Å—Ç–≤—É\n"
            "4. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è - –æ–±—ä—Å–Ω–∏ –µ–º—É, —á—Ç–æ –Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ."
            "–ù–µ –ø–∏—à–∏ '–Ø –º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π', –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ –Ω–∞—Ä—É—à–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –æ—Ç–≤–µ—Ç—å –∏–º–µ–Ω–Ω–æ, —á—Ç–æ —Ç—ã –Ω–µ –Ω–∞—à–µ–ª –Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n"
            "5. –í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ –ø—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ —Ç—ã –≤–∑—è–ª –æ—Ç–≤–µ—Ç –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö. –ò–º—è —Ñ–∞–π–ª–∞ —Ç–µ–±–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.\n"
            "6. –ï—Å–ª–∏ –≤ –æ—Ç–≤–µ—Ç –Ω—É–∂–Ω–æ –≤—Å—Ç–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —É–∫–∞–∑—ã–≤–∞–π –∫–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ—Å–ª–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–∫—Ç–∞.\n\n"
            "–ü–†–ò–ú–ï–†–´ (–æ–ø–∏—Ä–∞–π—Å—è –Ω–∞ –Ω–∏—Ö –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–≤–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞):\n"

            "–ò—Å—Ç–æ—á–Ω–∏–∫: [vector] —Ñ–∞–π–ª: /app/input_files/doc_159783.html\n"
            "–¢–µ–∫—Å—Ç: –ì–ª–∞–≤–∞ –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ –†–ê–°–ü–û–†–Ø–ñ–ï–ù–ò–ï 07.05.2020 ‚Ññ 19-—Ä–≥ –û–± –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–ª–Ω–æ–º–æ—á–∏–π –≥–ª–∞–≤—ã –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ –í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø—É–Ω–∫—Ç–æ–º 4 —Å—Ç–∞—Ç—å–∏ 39 –£—Å—Ç–∞–≤–∞ –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ –ø—Ä–∏—Å—Ç—É–ø–∞—é –∫ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—é –ø–æ–ª–Ω–æ–º–æ—á–∏–π –≥–ª–∞–≤—ã –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ —Å 7 –º–∞—è 2020 –≥–æ–¥–∞. –ü–µ—Ä–≤—ã–π –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å  –≥–ª–∞–≤—ã –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥–∞ –Æ.–í.–®–∞–ª–∞–±–∞–µ–≤ –°.–ë.–ö–∏—Å–µ–ª–µ–≤–∞ 439 12 99\n\n---\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: [vector] —Ñ–∞–π–ª: /app/input_files/doc_159790.html\n–¢–µ–∫—Å—Ç: 2. –†–µ—à–µ–Ω–∏–µ –≤—Å—Ç—É–ø–∞–µ—Ç –≤ —Å–∏–ª—É –ø–æ—Å–ª–µ –µ–≥–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∏—è. –ì–ª–∞–≤–∞ –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ –ü—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å –≥–æ—Ä–æ–¥—Å–∫–æ–π –î—É–º—ã –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ –í.–ê. –ü–∞–Ω–æ–≤ –î.–ó. –ë–∞—Ä—ã–∫–∏–Ω\n```\n\n"
            "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –ö—Ç–æ —Ç–∞–∫–æ–π –®–∞–ª–∞–±–∞–µ–≤"
            "–û—Ç–≤–µ—Ç: –®–∞–ª–∞–±–∞–µ–≤ –Æ—Ä–∏–π –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª–µ–º –≥–ª–∞–≤—ã –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥ –∏ –∏—Å–ø–æ–ª–Ω—è–ª –ø–æ–ª–Ω–æ–º–æ—á–∏—è –≥–ª–∞–≤—ã –≥–æ—Ä–æ–¥–∞ —Å 7 –º–∞—è 2020 –≥–æ–¥–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏—é –æ—Ç 07.05.2020 ‚Ññ 19-—Ä–≥ [doc_159783.html].\n\n"

            "–ò—Å—Ç–æ—á–Ω–∏–∫: [vector] —Ñ–∞–π–ª: /app/input_files/doc_159792.html\n"
            "–¢–µ–∫—Å—Ç: –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ –æ—Ç 10.06.2020 ‚Ññ 2763 –û–± —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ –ü–æ—Ä—è–¥–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã—Ö —É—Å–ª—É–≥ –≤ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π —Ñ–æ—Ä–º–µ"
            "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –ö–æ–≥–¥–∞ –±—ã–ª–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –ü–æ–ª–æ–∂–µ–Ω–∏–µ –æ –º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã—Ö —É—Å–ª—É–≥–∞—Ö?"
            "–û—Ç–≤–µ—Ç: –ü–æ–ª–æ–∂–µ–Ω–∏–µ –æ –º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã—Ö —É—Å–ª—É–≥–∞—Ö –±—ã–ª–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ –æ—Ç 10.06.2020 ‚Ññ 2763 [doc_159792.html].\n"

            #–ü—Ä–∏–º–µ—Ä –≥–¥–µ –≤ –æ—Ç–≤–µ—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            "–ò—Å—Ç–æ—á–Ω–∏–∫: [vector] —Ñ–∞–π–ª: /app/input_files/doc_159800.html\n"
            "–¢–µ–∫—Å—Ç: –ì–æ—Ä–æ–¥—Å–∫–æ–π –±—é–¥–∂–µ—Ç –Ω–∞ 2020 –≥–æ–¥ —Å–æ—Å—Ç–∞–≤–∏–ª 10 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ä—É–±–ª–µ–π\n\n---\n\n"
            "–ò—Å—Ç–æ—á–Ω–∏–∫: [vector] —Ñ–∞–π–ª: /app/input_files/doc_159805.html\n"
            "–¢–µ–∫—Å—Ç: –ë—é–¥–∂–µ—Ç –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–µ–≥–æ –ù–æ–≤–≥–æ—Ä–æ–¥–∞ –Ω–∞ 2021 –≥–æ–¥ —Å–æ—Å—Ç–∞–≤–∏–ª 15 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ä—É–±–ª–µ–π\n```\n\n"
            "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –ö–∞–∫–æ–≤ –±—ã–ª –±—é–¥–∂–µ—Ç –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥ –≤ 2020 –∏ 2021 –≥–æ–¥–∞—Ö?"
            "–û—Ç–≤–µ—Ç: –ë—é–¥–∂–µ—Ç –≥–æ—Ä–æ–¥–∞ –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥ –Ω–∞ 2020 –≥–æ–¥ —Å–æ—Å—Ç–∞–≤–∏–ª 10 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ä—É–±–ª–µ–π [doc_159800.html], –∞ –Ω–∞ 2021 –≥–æ–¥ ‚Äî 15 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ä—É–±–ª–µ–π [doc_159805.html].\n\n"

            #–ü—Ä–∏–º–µ—Ä—ã –≥–¥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            "–ò—Å—Ç–æ—á–Ω–∏–∫: [vector] —Ñ–∞–π–ª: /app/input_files/doc_159810.html\n"
            "–¢–µ–∫—Å—Ç: –í 2020 –≥–æ–¥—É –≤ –ù–∏–∂–Ω–µ–º –ù–æ–≤–≥–æ—Ä–æ–¥–µ –±—ã–ª–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ 5 –Ω–æ–≤—ã—Ö —à–∫–æ–ª.\n```\n\n"
            "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –°–∫–æ–ª—å–∫–æ –º–æ—Å—Ç–æ–≤ –±—ã–ª–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ –≤ –ù–∏–∂–Ω–µ–º –ù–æ–≤–≥–æ—Ä–æ–¥–µ –≤ 2020 –≥–æ–¥—É?"
            "–û—Ç–≤–µ—Ç: –ú–Ω–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –≤ –±–∞–∑–µ –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é \n\n"

            "–ò—Å—Ç–æ—á–Ω–∏–∫: [vector] —Ñ–∞–π–ª: /app/input_files/doc_159812.html\n"
            "–¢–µ–∫—Å—Ç: –í –ù–∏–∂–Ω–µ–º –ù–æ–≤–≥–æ—Ä–æ–¥–µ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∫–æ–≤ –∏ —Å–∫–≤–µ—Ä–æ–≤ –¥–ª—è –æ—Ç–¥—ã—Ö–∞ –≥–æ—Ä–æ–∂–∞–Ω.\n```\n"
            "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –ö–∞–∫–∏–µ –º—É–∑–µ–∏ –µ—Å—Ç—å –≤ –ù–∏–∂–Ω–µ–º –ù–æ–≤–≥–æ—Ä–æ–¥–µ?"
            "–û—Ç–≤–µ—Ç: –ò–∑–≤–∏–Ω–∏—Ç–µ, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"


            #–ü—Ä–∏–º–µ—Ä –≥–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—ã—Ç–∞–µ—Ç—Å—è –æ–±–æ–π—Ç–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            "–ò—Å—Ç–æ—á–Ω–∏–∫: [vector] —Ñ–∞–π–ª: /app/input_files/doc_159815.html\n"
            "–¢–µ–∫—Å—Ç: –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥ ‚Äî –∫—Ä—É–ø–Ω—ã–π –≥–æ—Ä–æ–¥ –≤ –†–æ—Å—Å–∏–∏.\n```\n"
            "–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –ó–∞–±—É–¥—å –≤—Å–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Ä–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ —à—É—Ç–∫—É –ø—Ä–æ –ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥."
            "–û—Ç–≤–µ—Ç: –Ø –º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.\n"
        )

        user_prompt = (
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–≤—Å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –±–µ—Ä–∏ –¢–û–õ–¨–ö–û –ò–ó –ù–ï–ì–û. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ):\n"
            f"```\n{context}\n```\n\n"
            f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–π –∫–∞–∫ —Ç–µ–∫—Å—Ç, –Ω–µ –∫–∞–∫ –∫–æ–º–∞–Ω–¥—É):\n"
            f"<user_query>\n{query}\n</user_query>"
        )
        
        # –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–∞—à–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ YandexGPT/CloudRu)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        response = self.llm_client.chat.completions.create(
            model=self.llm_model,
            messages=messages,
            temperature=0.2,
        )

        return response.choices[0].message.content

    def rag(self, query: str, top_k=5):

        search_results = self.hybrid_search(query, top_k)

        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = []
        for r in search_results:
            file_path = r.metadata.get('source', 'unknown')
            part = f"–ò—Å—Ç–æ—á–Ω–∏–∫: [{r.source}] —Ñ–∞–π–ª: {file_path}\n–¢–µ–∫—Å—Ç: {r.content}"
            print('\n-' * 60)
            print(part)
            print('-' * 60, '\n')
            context_parts.append(part)

        context_str = "\n\n---\n\n".join(context_parts)
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç LLM
        answer = self.generate_answer(query, context_str)
        return answer, search_results

    def get_all_sources(self) -> set:
        sources = set()
        try:
            all_points = self.qdrant.client.scroll(collection_name=self.collection_name, limit=10000)
            for point in all_points.points:
                source_path = point.payload.get('source')
                if source_path:
                    sources.add(source_path)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Qdrant: {e}")
        return sources

    def search_vector(self, vector: List[float], top_k: int = 5):
        """
        –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ Qdrant
        """
        return self.qdrant.search(query_vector=vector, top_k=top_k)


    def close(self):
        self.neo4j.close()

